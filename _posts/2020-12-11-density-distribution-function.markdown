---
layout: post
title: "Density and Distribution Function"
date: 2020-12-11 01:30:06 +0800
img : density.png
tags: [probability, measure]
---

Defining the cumulative distribution function uniquely defines the probability measure for the whole measure space as we shall demonstrate in this post.

<div class="toc" markdown="1">
# Contents:
- [Cumulative Distribution Function](#cdf)
- [Joint Cumulative Distribution Function](#joint)
- [Radon-Nikodym Theorem](#radon)
- [Discrete Random Variable Distribution](#discrete)
- [Continuous Random Variable Distribution](#continuous)
- [Conditional PDF](#conditional)
</div>

## <a name="cdf"></a>Cumulative Distribution Function

Recall that the probability measure $$P_{X}$$ induced by the measure $$P$$ in the measure space $$(\Omega, \mathcal{F}, P) $$ is defined as:

$$\begin{aligned}
P_{X}(A') &= P[X^{-1}(A')]\\
\forall A' &\in \mathcal{F}'_{X}
\end{aligned}$$

in measure space $$(\Omega', \mathcal{F}', P_{X}) $$.

Consider a continuous random variable of the measurable space $$(\mathbb{R}, \mathcal{B})$$. We know that half-open intervals of the form $$(-\infty, x]$$ are Borel sets as it is a generating set of Borel $$\sigma-$$algebra:

$$\begin{aligned}
\sigma[(-\infty, x] : x \in \mathbb{R}] &= \mathcal{B}\\
\end{aligned}$$

Hence $$P_{X}[(-\infty, x]]$$ are well defined $$\forall x \in \mathbb{R}$$:

$$\begin{aligned}
P_{X}[(-\infty, x]] &= P(\{\omega \in \Omega : X(\omega) \leq x\})\\
&:= F_{X}(x)\\
\end{aligned}$$

$$F_{X}$$ is known as the cumulative distribution function (CDF).

### $$\pi$$-System

A collection $$\mathcal{G}$$ of subsets of $$\Omega$$ is called a $$\pi$$-system and closed under finite intersection if:

$$\begin{aligned}
A \cap B &\in \mathcal{G}\\
\forall A, B &\in \mathcal{G}
\end{aligned}$$

All $$\sigma$$-algebras are $$\pi$$-system, but the converse is not true as $$\pi$$-system is weaker than $$\sigma$$-algebra. However, if a probability measure's $$\sigma$$-algebra agreed with the $$\sigma$$-algebra generated by $$\mathcal{G}$$, then they have the same probability measure.

Given measure spaces $$(\Omega, \mathcal{F}_{1}, P_{2})$$ and $$(\Omega, \mathcal{F}_{2}, P_{2})$$ if:

$$\begin{aligned}
\mathcal{F}_{1} &= \sigma(\mathcal{G})\\
\mathcal{F}_{2} &= \sigma(\mathcal{G})\\
\text{then } &P_{1} = P_{2}
\end{aligned}$$

The half-open intervals are in fact a $$\pi$$-system:

$$\begin{aligned}
\pi(\mathbb{R}) := \{(-\infty, x] &: x \in \mathbb{R}\} = \mathcal{G}\\
\sigma(\pi(\mathbb{R})) &= \mathcal{B}(\mathbb{R})\\
\end{aligned}$$

Hence, defining the probability CDF uniquely specifies the probability measure for the whole Borel set $$\mathcal{B}(\mathbb{R})$$ and hence the whole $$\sigma$$-algebra $$\mathcal{F}'$$.

### Properties of CDF

(a) End limits:

$$\begin{aligned}
\lim\limits_{x \rightarrow \infty} F_{X}(x) = 1\\
\lim\limits_{x \rightarrow -\infty} F_{X}(x) = 0\\
\end{aligned}$$

Let $$A_{i} = \{\omega : X(\omega) \leq x_{i} \}$$. Hence, $$P(A_{i}) = F_{X}(x_{i})$$.

If $$A_{i}$$ is an increasing sequence $$A_{i} \subset A_{i + 1}$$ then using continuity of measure:

$$\begin{aligned}
\lim\limits_{i \rightarrow \infty} P(A_{i}) &= P(\cup_{i = 1}^{\infty})\\
&= P(A_{\infty})\\
&= P(\Omega)\\
&= 1
\end{aligned}$$

Similarly, if $$A_{i}$$ is a decreasing sequence $$A_{i} \supset A_{i + 1}$$ then using continuity of measure:

$$\begin{aligned}
\lim\limits_{i \rightarrow \infty} P(A_{i}) &= P(\cap_{i = 1}^{\infty})\\
&= P(A_{-\infty})\\
&= P(\emptyset)\\
&= 0
\end{aligned}$$

(b) Monotonicity:

$$\begin{aligned}
F_{X}(x) &\leq F_{X}(y)\\
\forall x &\leq y
\end{aligned}$$

Since:

$$\begin{aligned}
\{w : X(\omega) \leq x \} &\subset \{w : X(\omega) \leq y \}\\
P(\{w : X(\omega) \leq x \}) &\leq P(\{w : X(\omega) \leq y \})\\
\end{aligned}$$

(c) Right continuous:

$$\begin{aligned}
\lim\limits_{\epsilon \rightarrow 0^{+}} F_{X}(x + \epsilon) = F_{X}(x)\\
\end{aligned}$$

In other words $$F_{X}$$ is allowed to have jumps but need to be right continuous.

Let $$\{\epsilon_{i} : \epsilon_{i} \geq 0\}$$ be a decreasing sequence such that $$\lim\limits_{i \rightarrow \infty} \epsilon_{i} = 0$$. 

And using continuity of measure again:

$$\begin{aligned}
\lim\limits_{i \rightarrow \infty} F_{X}(x + \epsilon_{i}) &= \lim\limits_{i \rightarrow \infty} P(X \leq x + \epsilon_{i})\\
&= P(\cap_{i = 1}^{\infty} \{w : X(\omega) \leq x + \epsilon_{i} \})\\
&= P(\{w : X(\omega) \leq x + \epsilon_{\infty} \})\\
&= P(\{w : X(\omega) \leq x \})\\
&= F_{X}(x)
\end{aligned}$$

If we were to approach from the left then it will be an increasing sequence and using continuity of measure:

$$\begin{aligned}
\lim\limits_{i \rightarrow \infty} F_{X}(x - \epsilon_{i}) &= \lim\limits_{i \rightarrow \infty} P(X \leq x - \epsilon_{i})\\
&= P(\cup_{i = 1}^{\infty} \{w : X(\omega) \leq x - \epsilon_{i} \})\\
&= P(\{w : X(\omega) \leq x - \epsilon_{\infty} \})\\
&= P(\{w : X(\omega) \leq x \}) - P(\{w : X(\omega) = x \})\\
&= P(\{w : X(\omega) < x \})\\
&= F_{X}(x^{-})\\
&= F_{X}(x) - P(X = x)\\
\end{aligned}$$

So we can see the right continuous property is a result of the conventional definition of $$F_{X}(x) := P(X \leq x)$$.

If $$F_{X}(x) := P(X < x)$$ then it will be left continuous rather than right continuous. 

If there is a discontinuity jump at $$x$$, then $$P(X = x) > 0$$. This would especially be the case for discrete random variable. The jump will be equivalent of the probability of the random variable at $$X = x$$.

Furthermore, given these properties, it is enough to derive a unique cdf function. In other words, given any function that satisfy the above properties, it will be a unique and valid cdf for a particular random variable.

## <a name="radon"></a>Radon Nikodym Theorem

Let $$X$$ be a continuous random variable and $$P_{X}$$ be a absolute continuous w.r.t to the Lebesgue measure $$P_{X} \ll_{\mathcal{F}} \lambda$$. Then there exists a non-negative measurable mapping $$f_{X}: \mathbb{R} \Rightarrow [0, 1]$$ called a probability density function (PDF) such that:

$$\begin{aligned}
P_{X}(B) &= \int_{B}f_{X}d\lambda\\
\forall B &\in \mathcal{B}(\mathbb{R})\\
\end{aligned}$$

In particular,

$$\begin{aligned}
P_{X}[(-\infty, x]] &= F_{X}(x)\\
&= \int_{-\infty}^{x}f_{X}(x)dx\\
\forall x &\in \mathbb{R}
\end{aligned}$$

In other words, there exists a measurable function who's integral is the CDF. In fact the above is often used to define a continuous random variable and this is compatible to the definition given [here]({% post_url 2020-12-06-measurable-mapping-random-variable %}#decomposition).

And the Radon Nikodym derivative is defined as:

$$\begin{aligned}
\frac{dP_{X}(B)}{d\lambda} &= f_{X}\\
\forall B &\in \mathcal{B}(\mathbb{R})\\
\end{aligned}$$

## <a name="discrete"></a>Discrete Random Variable Distribution

Recall in the previous [post]({% post_url 2020-12-06-measurable-mapping-random-variable %}) where we defined the discrete random variable probability mass function as:

$$\begin{aligned}
p_{X}(x) &:= P_{X}(\{x\})\\
&:= P(X = x)
\end{aligned}$$

Here are some examples of discrete random variable distributions.

### Bernoulli Distribution

Let $$E = \{0, 1\}$$. 

Given $$0 \leq p \leq 1$$, the Bernoulli pmf is defined as:

$$\begin{aligned}
p_{X}(0) &= 1 - p\\
p_{X}(1) &= p\\
\end{aligned}$$

An example is the probability of getting head in a coin toss $$p = P(H)$$.

Note that this is equivalent to:

$$\begin{aligned}
p &= p_{X}(1)\\
&= P_{X}(\{1\})\\
&= P(X^{-1}(\{1\}))\\
&= P(H)\\
\end{aligned}$$

However when we do computation using the $$P_{X}$$ measure, we do not have to refer back to the domain measure space $$(\Omega, \mathcal{F}, P)$$.

### Uniform Distribution

Let $$E = \{e_{1},\: \cdots, e_{n}\}$$ be a finite set.

The uniform pmf is defined as:

$$\begin{aligned}
&p_{X}(e_{i}) = \frac{1}{n}\\
&\forall i = 1, 2,\: \cdots, n\\
\end{aligned}$$

Note that uniform pmf can only be defined for finite sets and not countably infinite sets.

### Geometric Distribution

Let $$E = \mathbb{N}$$.

The geometric pmf is defined as:

$$\begin{aligned}
p_{X}(k) &= (1 - p)^{k - 1}p\\
\forall k &\in \mathbb{N}\\
p &\in [0, 1]\\
\end{aligned}$$

An example will be $$p = P(H)$$ and interpreted as the probability of seeing the first H after the first $$k$$ independent tosses.

It is the only discrete distribution that has the following memoryless property:

$$\begin{aligned}
P(X > m + n &\mid X \geq m) = P(X > n)\\
\forall\: m, n &\in \{0, 1, 2,\: \cdots \}\\
\end{aligned}$$

### Binomial Distribution

Let $$E = \{0, 1, 2,\: \cdots, n\}$$.

The binomial pmf is defined as:

$$\begin{aligned}
p_{X}(k) &= {n \choose k}p^{k}(1-p)^{(n - k)}\\
k &\in E\\
p &\in [0, 1]\\
\end{aligned}$$

An example will be $$p = P(H)$$ and interpreted as the probability of seeing $$k$$ H out of $$n$$ independent tosses.

### Poisson Distribution

Let $$E = \{0, 1, 2,\: \cdots\}$$.

The Poisson pmf is defined as:

$$\begin{aligned}
p_{X}(k) &= \frac{e^{-\lambda}\lambda^{k}}{k!}\\
k &\in E\\
\lambda &> 0\\
\end{aligned}$$

An example will be $$\lambda =$$ the rate of deaths by horse kicks.

## <a name="continuous"></a>Continuous Random Variable Distribution

Using Radon Nikodym Theorem we defined the continuous random variable probability density function w.r.t the CDF as:

$$\begin{aligned}
P_{X}[(-\infty, x]] &= F_{X}(x)\\
&= \int_{-\infty}^{x}f_{X}(x)dx\\
\forall x &\in \mathbb{R}
\end{aligned}$$

Here are some examples of continuous random variable distributions.

### Uniform Distribution

Given the measurable space $$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$$, the uniform CDF and PDF on $$[a, b]$$ is defined as:

$$\begin{alignedat}{3}
F_{X}(x) &= \begin{cases}
0 &x < a,\\
\frac{x - a}{b - a} &a \leq x \leq b,\\
1 &x > b\\
\end{cases}
\\[20pt]
f_{X}(x) &= \begin{cases}
\frac{1}{b - a}\: &a \leq x \leq b,\\
0 &\text{otherwise}
\end{cases}
\end{alignedat}$$

### Exponential Distribution

Given the measurable space $$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$$, the exponential CDF and PDF is defined as:

$$\begin{alignedat}{2}
F_{X}(x) &= \begin{cases}
1 - e^{-\lambda} &x \geq 0,\\
0 &x < 0\\
\end{cases}
\\[20pt]
f_{X}(x) &= \begin{cases}
\lambda e^{\lambda x}\:\:\:\:\:\: &x \geq 0,\\
0 &x < 0\\
\end{cases}
\end{alignedat}$$

It is the only continuous distribution that has the following memoryless property:

$$\begin{aligned}
P(X > s + t \mid X > t) &= P(X > s)\\
\forall\: s, t &\geq 0\\
\end{aligned}$$

### Standard Cauchy Distribution

Given the measurable space $$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$$, the standard Cauchy CDF and PDF is defined as:

$$\begin{aligned}
F_{X}(x) &= \frac{1}{\pi}tan^{-1}(x) + \frac{1}{2}\\[5pt]
f_{X}(x) &= \frac{1}{\pi}\frac{1}{1 + x^{2}}\\
\forall x &\in \mathbb{R}
\end{aligned}$$

Compared to Gaussian distribution, Cauchy distribution falls off much slower $$x^{-2}$$ vs $$e^{-x^{2}}$$ (power law vs exponential).

### Gaussian Distribution

Given the measurable space $$(\mathbb{R}, \mathcal{B}(\mathbb{R}))$$, the Gaussian CDF and PDF is defined as:

$$\begin{aligned}
F_{X}(x) &= \frac{1}{2}[1 + \frac{1}{2 \sqrt{\pi}}\int_{-\infty}^{\frac{x - \mu}{\sigma \sqrt{2}}}e^{-t^{2}}dt]\\
&= \frac{1}{2}[1 + erf(\frac{x - \mu}{\sigma \sqrt{2}})]\\[5pt]
f_{X}(x) &= \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x - \mu)^{2}}{2 \sigma^{2}}}\\
\forall x &\in \mathbb{R}
\end{aligned}$$

The error function $$erf()$$ is defined as the CDF of a standard Gaussian distribution:

$$\begin{aligned}
F_{X}(x) &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-t^{2}}dt\\
&= erf(x)\\[5pt]
f_{X}(x) &= \frac{1}{\sqrt{2 \pi}}e^{-\frac{x^{2}}{2}}\\
\forall x &\in \mathbb{R}
\end{aligned}$$

## <a name="joint"></a>Joint Cumulative Distribution Function

For $$\mathbb{R}^{2}$$ the joint CDF is defined as:

$$\begin{aligned}
F_{X, Y}(x, y) := &P_{X, Y}\big((-\infty, x] \times (-\infty, y]\big)\\
= &P(\{\omega \mid X(\omega) \leq x\} \cap \{\omega \mid Y(\omega) \leq y\})\\
= &P(X \leq x, Y \leq y)\\
\end{aligned}$$

### Properties of Joint CDF

(a) End limits:

$$\begin{aligned}
\lim\limits_{(x, y) \rightarrow (\infty, \infty)} F_{X, Y}(x, y) = 1\\
\lim\limits_{(x, y) \rightarrow (-\infty, -\infty)} F_{X, Y}(x, y) = 0\\
\end{aligned}$$

(b) Monotonicity:

$$\begin{aligned}
F_{X, Y}(x_{1}, y_{1}) &\leq F_{X, Y}(x_{2}, y_{2})\\
\forall (x_{1}, y_{1}) &\leq (x_{2}, y_{2})\\
\end{aligned}$$

(c) Continuity from above:

$$\begin{aligned}
\lim\limits_{(\epsilon, \delta) \rightarrow (0^{+}, 0^{+})} F_{X, Y}(x + \epsilon, y + \delta) = F_{X, Y}(x, y)\\
\end{aligned}$$

(d) Marginal CDF:

$$\begin{aligned}
\lim\limits_{x \rightarrow \infty} F_{X, Y}(x, y) = F_{Y}(y)\\
\lim\limits_{y \rightarrow \infty} F_{X, Y}(x, y) = F_{X}(x)\\
\end{aligned}$$

## <a name="conditional"></a>Conditional PDF

The conditional CDF of $$X$$ given $$Y$$ is defined as:

$$\begin{aligned}
F_{X \mid Y}(x \mid y) := &\frac{\int_{-\infty}^{a}f_{X, Y}(x, y)dx}{f_{Y}(y)}\\
\end{aligned}$$

And the conditional PDF can be derived from the conditional CDF as:

$$\begin{aligned}
f_{X \mid Y}&= \frac{f_{X, Y}(x, y)}{f_{Y}(y)}\\
\end{aligned}$$

The conditional probablity of an event $$\{X \in A\}$$, given $$Y$$ is defined as:

$$\begin{aligned}
P(X \in A \mid Y = y) := &\int_{A}f_{X \mid Y}(x \mid y)dx\\
= &\int_{-\infty}^{\infty}I_{A}(x)f_{X \mid Y}(x \mid y)dx\\
\end{aligned}$$
